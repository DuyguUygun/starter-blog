---
title: "Trust and criticism in science, Part II: Technological extension"
date: 2020-10-29T16:42:13.402Z
draft: false
featured: false
image:
  filename: featured
  focal_point: Smart
  preview_only: false
---
<!--StartFragment-->

 

In the first part I have talked about where and when trust might play an ineliminable role in science, in contrast to the prevalent opinion that scientific inquiry essentially requires a skeptical attitude. 

I maintained that the problematic aspect of trust is that it generates *epistemic vulnerability*, which we clearly should not tolerate in the context of scientific knowledge production, and not that it involves *epistemic dependence* — that is, reliance on external sources such as testimony for justification. I argued that trust can be irrational to the extent that it implies epistemic vulnerability, but it can also be rational if it is based on justified (second-order) beliefs about the trustworthiness or reliability of an epistemic source and if it is constantly calibrated. In the scientific context such beliefs are justified to the extent that the research findings behind a scientific assertion have high *inquirability* and the social process of *criticism*, or scientific quality control, is working reliably. I described research findings as inquirable to the extent that the process whereby they are generated is transparent, the data are available, the assumptions regarding the reliability and validity of the measures are visible, the methods are repeatable by independent others and so on. Inquiribility thus allows that empirical evidence can easily be subjected to criticism. The social process of criticism, on the other hand, is reliable to the extent that it is generally able to detect errors, evidential inadequacies, weak inferential connections and so on, when such are present. I argued that inquirability facilitates and proliferates criticism, and actual criticism in turn raises the bar on accepted standards for inquirability. Moreover, both the inquirability of scientific research and the actual processes of criticism can be scaffolded by technologies of cognitive extension and distribution of epistemic labor and credit, which imply that we partly rely on external epistemic sources in judging the reliability of (various aspects of) scientific inquiry. 

Thus, I concluded, we have to minimize the role of trust in science where it implies epistemic vulnerability, but not necessarily epistemic dependence — because we often need to become less self-reliant in order to make our processes of knowledge production more reliable, hence science as a whole more trustworthy. When the trustworthiness of the scientific enterprise comes into question, what we have to increase is inquirability and criticism, not skepticism per se, so that we can cultivate well-justified, rational trust.

In this part I focus on how technologies of cognitive extension can play a significant role in increasing the inquirability of research and rendering the social process of criticism more efficient and reliable. 

\*\**

To take off from where I left with a summary description of the current states of affairs in the first part, thus to recap why we should be talking about the issues of trust and criticism in science in the first place, there are increasingly vocal concerns that the quality control mechanism of science in several fields is not working adequately, if not broken. The core of this concern is clearly that epistemic vulnerability is prevalent, thus trust in scientific findings is not justified.

There are several core factors contributing to the prevalence of epistemic vulnerability where this trust is not adequately justified:

(i) One factor is the increased scale of science. Contemporary science has a quite different outlook than the science back in the early modern and modern times. While in the past scientific production as well as quality control were in the hands of a much smaller group of people, now science has become much more democratic in its production, if not equally so in its criticism. In the past the role authoritative scientific institutions played in the criticism of scientific claims was quite decisive. For instance, in its earlier days a central mission of the Royal Society (and of similar institutions such as the Accademia del Cimento) was to design and conduct replication studies to asses the credibility of reported findings (reference). Now the actual rate of replications is unsurprisingly much lower due (among other things) to the sheer number of published findings. Another crucial element of scientific criticism, [peer-review has never been a fail-safe process](https://www.sciencealert.com/these-8-papers-were-rejected-before-going-on-to-win-the-nobel-prize) to acknowledge solid research and filter out weak reports, but now we hear more than ever before that [peer-review does not fulfill its purpose of quality control](https://academic.oup.com/bjps/advance-article/doi/10.1093/bjps/axz029/5526887) and should better be abolished in its traditional form.

(ii) Another factor is the ever increasing complexity of contemporary science. Today even in the so-called softer fields of science the nature of the produced knowledge is often computational and require highly specialized skill sets for its evaluation. As the technologies of data gathering, data analysis, model building and testing become more complex, novel phenomena enter the purview of science (such as patterns in big data) and it becomes to that extent more difficult to evaluate scientific outputs using naked human cognition. Consequently, the evaluation of scientific findings requires much more resources than in the past, which we need to shift from the domain of discovery but not seldomly prefer not to due to unfavourable incentive structures. 

(iii) One other, relatively constant factor is the tragic mismatch between the represented epistemic subject (the model researcher) of most “rationalist” schools of philosophy of science and the actual epistemic subjects, whose decisions are studied more accurately by, for instance, social psychology than by the logic and methodology of science. When the social mechanisms for scientific quality control are inadequate or failing, epistemic weaknesses or faults of individuals become crucial determinants of the epistemic value of scientific claims.

Thus, the original scientific context in which the social practices and technologies of criticism that we still rely on today (such as the traditional peer-review) were developed was quite different than the contemporary one, but we have invested hardly enough resources to address this mismatch.

Technological extension of cognitive skills can indeed help significantly in mitigating the effect of these and similar sources of epistemic vulnerability. Let us begin with a brief exposition of the concept of extended cognition.

\*\**

In their classic [paper](https://draft.blogger.com/#) on the topic, Andy Clark and David Chalmers define extended cognition in terms of an epistemic parity consideration:

> Epistemic action, we suggest, demands spread of epistemic credit. If, as we confront some task, a part of the world functions as a process which, were it done in the head, we would have no hesitation in recognizing as part of the cognitive process, then that part of the world is (so we claim) part of the cognitive process. Cognitive processes ain’t (all) in the head!

Their famous case, Otto’s notebook, illustrates an extended memory retention and retrieval process. Otto suffers from Alzheimer’s disease and always carries with him a notebook to keep all necessary information. He regularly adds new content to the notebook and relies on the information recorded in the notebook whenever he needs to recall something. Clark and Chalmers argue that Otto’s notebook functions just as an biological (onboard) memory functions: He carries it always with him, he consults it whenever he needs to recall a piece of information, and he accepts the information recorded in it more or less automatically — just as we do when we recall from biological memory. Thus, they maintain, when we deny that Otto’s notebook is truly a part of a cognitive process, we show unjustified bias towards processes taking place inside the head. 

Their view of extended cognition suggests some conditions for a resource (e.g. artifact) to be a *constitutive* part of a cognitive process. [Clark](https://draft.blogger.com/#) summarizes these in three items he calls the “trust and glue” conditions:

> (i) The resource is reliably available and typically invoked.
>
> (ii) Any information retrieved or gained via it should be more-or-less automatically endorsed. It should not usually be subject to critical scrutiny.
>
> (iii) The information contained should be easily accessible when required.

From this perspective, depending on the extent to which we *rely on* them in realizing cognitive processes, a vast variety of artifacts from implants to digital devices, software and data repositories can become part of a cognitive system extended beyond the boundaries of skin and skull. “In these cases,” [Clark and Chalmers](https://draft.blogger.com/#) argue, “the human organism is linked with an external entity in a two-way interaction creating a coupled system that can be seen as a cognitive system in its own right.”

The functions of cognitive artifacts are not limited to *substituting* a biological counterpart or allowing us to *outsource* cognitive labor. Through being coupled with our biological cognitive processes they can *alter* the nature of the cognitive task, *amplify* our cognitive performance or enable us to realize *novel* cognitive processes which lie beyond our mere biological cognitive capacities. 

In numerous rather mundane cases human reasoners rely heavily on environmental resources in realizing cognitive tasks, such as when one randomly re-arranges scrabble tiles in order to better recognize possible meaningful arrangements or uses pen and paper to do complex calculations. As [Richard Menary](https://draft.blogger.com/#) maintains, such cases illustrate a change in the *task space* through the manipulation of external vehicles, for instance from one chiefly involving verbal memory to one equally involving perception and sensory-motor coordination.

In the context of scientific research there are numerous examples of coupled agent-artifact systems that realize processes of observation, data analysis, modelling, measurement, simulation etc. that mere human agents would find either extremely difficult or outright impossible to do. The way in which super computers, space telescopes, advance algorithms, machine learning or various other technologies are used in scientific discovery often goes beyond outsourcing of cognitive labor, as it is the case when someone constantly uses a calculator to perform even relatively simple arithmetic tasks, and illustrate a form of coupling that makes certain highly difficult or altogether novel kinds of epistemic achievements possible. 

\*\**

Now, can extended cognition yield adequate epistemic justification, thus (possibly) knowledge? Can someone claim to know via an extended process such as one involving the calculation of a function using software just as one can claim to know that there is a cat on the mat because he saw it and has reliable vision? Or does extended cognition involve too high a degree of epistemic dependence and consequently not sufficient contribution from the agent? Moreover, is such epistemic dependence compatible with epistemic responsibility or implies a high propensity of epistemic vulnerability? 

These are similar questions are clearly not only relevant from a purely epistemological perspective but also in relation to scientific criticism, reliability of processes of scientific inquiry, and the nature of scientific knowledge. 

Our conception of knowledge is guided, according to [Duncan Pritchard](https://draft.blogger.com/#), by an “ability intuition;” namely, believing truly can count as knowledge only if the exercise of a pertinent epistemic ability or disposition on the agent’s part is chiefly or significantly responsible for the belief’s being true. In relation to the case of extended cognition Pritchard [argues](https://draft.blogger.com/#) that an extended cognitive process can count as a genuine ability or disposition only if it is reliable (that is, it yields true beliefs much more often than it errs) and appropriately integrated into the agent’s “cognitive character;” namely, an agent’s “integrated web of stable and reliable belief-forming processes.” Integration of an extended cognitive process into the cognitive character of an agent requires, for Pritchard, that the agent knows the *source* of the reliability of the process in question and *that* it is reliable.

In the previous discussion on well-justified, rational trust I had maintained that one does not necessarily need to possess first-order evidence in order to know, but can also do so by relying on an external epistemic source if one has second-order evidence that the source has the pertinent first-order evidence. Similarly, I argue that appropriate integration into one’s cognitive character does not necessarily require that one has first-order evidence that an extended cognitive process is reliable and is adequately familiar with the source of its reliability. Especially in the context of scientific inquiry, often researchers do not have adequate familiarity with the theories, models or empirical evidence behind a complex cognitive artifact that they are heavily relying on in their own research. For instance a linguist whose research depends on a machine learning algorithm to extract information from vast databases of written text does not have to possess the proper expertise to program, modify or scrutinize it in order to have good reasons to believe that the information extraction process is reliable, as often there would be other experts in her extended network or possibly even some she is collaborating with who have good evidence for the reliability of the process and sufficient understanding of the sources of its reliability. Thus her reliance on the cognitive artifact would nonetheless be rational and responsible, although there would be less conscious cognitive engagement or agentive scrutiny on her part. 

Moreover, the guiding intuition behind the extended cognition thesis seems to require for genuine extension or coupling of systems (in contrast to merely using an external tool) a significant degree of epistemic dependence, or simply trust as appears in “trust and glue” conditions. Clark [underlines this point](https://draft.blogger.com/#) in reference to the case of Otto:

> As far as that argument goes, it should make no difference at all whether or not Otto is now, or ever was, aware of the source of the reliability of the notebook involving process. Indeed — and here comes the promised dilemma — there is a very real sense in which the more he is aware of such matters, the less the notebook will seem to be playing the same kind of functional role as biological memory. For as we noted, our biological memory is not typically subject to agentive scrutiny as a process at all, much less as one that may or may not be reasonably judged to be reliable by the agent.

Clark argues in favor of sub-personal forms of “epistemic hygiene” (e.g. unconscious meta-cognitive mechanisms) and against agentive vigilance, as he takes only the former to be compatible with genuine incorporation of an artifact into a cognitive system. But in the scientific context a more reliable and appropriate source of justification is simply the tightly connected epistemic network that supports the social process of scientific inquiry. One important aspect of saying that scientific knowledge is social knowledge is that scientific knowledge production involves division of epistemic labor in the form of specialization and collaboration. The incorporation of technologies of cognitive extension into processes of scientific inquiry takes place within a broader context of epistemic dependence on other researchers, as sources of justification as well as information. 

\*\**

Going back to our discussion of the asymmetrical development of technologies of scientific discovery and technologies of scientific criticism, which I maintained is one of the reasons behind the current prevalence of epistemic vulnerability, let us first look at technologies of scientific communication and its evaluation.

An important portion of the problem of the low rate of replicability[\[1]](https://draft.blogger.com/#)[\[2]](https://draft.blogger.com/#) or credibility[\[3]](https://draft.blogger.com/#) of scientific findings in several fields is due to widespread methodological weaknesses[\[4]](https://draft.blogger.com/#)[\[5]](https://draft.blogger.com/#), questionable research practices[\[6]](https://draft.blogger.com/#)[\[7]](https://draft.blogger.com/#) or breaches of scientific integrity[\[8]](https://draft.blogger.com/#), together with relatively low incentives for engaging in peer-review and conducting replication studies instead of fast and numerous original research[\[9]](https://draft.blogger.com/#)[\[10]](https://draft.blogger.com/#), but a significant role is played also by the fact that the current state of the technologies for increasing the inquirability of scientific outcomes and for faciliating scientific criticism leave much to be desired.

The main function of a scientific paper, the central element of scientific communication, is to report what has been observed clearly and transparently enough so that the readers can understand and in principle reproduce the whole observation process for themselves, and also judge whether they agree with the theoretical consequences drawn on the basis of the observation. However, the more complex the data collection, data analysis, the models, computations and inferential procedures behind the scientific report, the more difficult it becomes for the reader to assess the credibility of the results. In the current state, even thoroughly computational findings and highly complex theoretical inferences are communicated on simple printed paper or at best digital paper, the PDF. When the technologies of science communication lag behind those of scientific discovery, we have a potentially disasterous asymmetry, and scientific communication risks fulfilling its basic function.

Just as [linguistic symbols](https://draft.blogger.com/#) have worked as cognitive artifacts that changed the way we cognize, as [logical](https://draft.blogger.com/#) and [mathematical](https://draft.blogger.com/#) notations further radically expanded the scope of what is thinkable, the nature of the cognitive labor that goes into critically evaluating scientific outcomes can be dramatically altered through incorporating interactive diagrams, codes and algorithms into scientific publications. Moreover, the inquirability of research outcomes can rise incomparably if all the computational aspects of research are done in computational notebooks, which not only change the whole cognitive task space for the researcher but also can function as a form of scientific publication that ensures transparency of data, codes and methods. For instance [Mathematica](https://draft.blogger.com/#), a computational notebook designed by Stephen Wolfram already in 1988 offers a whole platform of cognitive artifacts to scaffold computational work for scientists:

> In Mathematica you can input a voice recording, run complex mathematical filters over the audio, and visualize the resulting sound wave; just by mousing through and adjusting parameters, you can warp the wave, discovering which filters work best by playing around.

Conducting and publishing research in computational notebooks would enable for the reader to see as well as interact with the whole computational process behind the reported findings, thus its inquirability:

> To write a paper in a Mathematica notebook is to reveal your results and methods at the same time; the published paper *and* the work that begot it. Which shouldn’t just make it easier for readers to understand what you did — it should make it easier for them to replicate it.

Fernando Pérez’s open source [IPyhton](https://draft.blogger.com/#) from 2000s (now continuing under project Jupyter which supports many more languages) carries the same idea much further by turning the notebook into “a computational partner, and as a thinking partner” and the process of using a computational notebook into a veritable process of extended cognition — both for the researcher and the others:

> The notebook walks through the work that generated every figure in the paper. Anyone who wants to can run the code for themselves, tweaking parts of it as they see fit, playing with the calculations to get a better handle on how each one works. 

A paper announcing the (first confirmed) detection of gravitational waves which was published additionally as an IPyhton notebook even allowed that

> the signal that generated the gravitational waves is processed into sound, and this you can play in your browser, hearing for yourself what the scientists heard first, the bloop of two black holes colliding.

Extending scientific cognition can scaffold scientific quality control in various other ways. For instance if tools for checking errors or inconsistencies in statistical results such as [Statcheck](https://draft.blogger.com/#) or [GRIM](https://draft.blogger.com/#) are fully incorporated into the routine practices of data analysis, the resulting extension of the required cognitive processes will increase their reliability significantly without necessarily requiring from the researcher any further expertise in statistics. A recent proposal to [make hypothesis tests machine readible](https://draft.blogger.com/#) further envisions computer-assisted checks of corroboration/falsification and meta-analyses. Such a reform in scientific reports could both facilitate much more effective and reliable criticism and compel researchers to meet higher demands of transparency, methodological rigor and intersubjective testability.

Extended cognition in scientific quality control would imply that beyond the availability of certain tools, the research is communicated and evaluated in a way that strongly depends on technical instruments. We could possibly arrive at a point where evaluating scientific outcomes becomes nearly impossible without adequate technological infrastructure, similarly to Otto’s external, inorganic memory, just as today scientific discovery is nearly impossible without pertinent technologies.

Thus the way to make science more reliable and credible does not only pass through increasing self-reliance and skeptical vigilance, but possibly also through increasing our epistemic dependence on technological artifacts and other fellow scientists. This second path is also more preferable, because it addresses the asymmetry between incentives for and technologies of scientific discovery and scientific criticism more realistically and efficiently, and it is much more fitting to the inherently social and increasingly extended nature of scientific knowledge.

In the third and last part I will complete the picture I so far sketched by talking about the distribution of scientific labor and credit over extended social networks of scientific collaboration. 

Originally published on [medium/science and philosophy] (https://medium.com/science-and-philosophy/trust-and-criticism-in-science-c7d85f4b301a)

[\[1]](https://draft.blogger.com/#) Camerer, C. F. et al. (2018). Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015. *Nature Human Behaviour*, *2*(9), 637–644. DOI: [https://doi.org/10.1038/s41562-018-0399-z](https://draft.blogger.com/#)

[\[2]](https://draft.blogger.com/#) Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. *Science*, *349*(6251), aac4716.

[\[3]](https://draft.blogger.com/#) Ioannidis, J. P. A. (2005) Why most published research findings are false. *PLoS Med*, *2*(8): e124.

[\[4]](https://draft.blogger.com/#) Ioannidis, J. P. A., Stanley, T. D., & Doucouliagos, H. (2017). The power of bias in economics research. *Econ J*, 127, F236-F265.

[\[5]](https://draft.blogger.com/#) Simmons J. P., Nelson L. D., Simonsohn U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. *Psychological Science*, 22(11):1359–1366. DOI: [https://doi.org/10.1177/0956797611417632](https://draft.blogger.com/#)

[\[6]](https://draft.blogger.com/#) John, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. *Psychological Science*, *23*(5), 524–532.

[\[7]](https://draft.blogger.com/#) Fraser, H., Parker, T., Nakagawa, S., Barnett, A., & Fidler, F. (2018). Questionable research practices in ecology and evolution. *PLoS ONE*, *13*(7): e0200303.

[\[8]](https://draft.blogger.com/#) Fanelli, D. (2009). How many scientists fabricate and falsify research? a systematic review and meta-analysis of survey data. *PLoS ONE, 4*(5): e5738.

[\[9]](https://draft.blogger.com/#) Tiokhin L. and Derex M. (2019). Competition for novelty reduces information sampling in a research game — a registered report*. R. Soc. open sci*.6180934. DOI: [https://doi.org/10.1098/rsos.180934](https://draft.blogger.com/#)

[\[10]](https://draft.blogger.com/#) Koole S. L., Lakens D. Rewarding replications: A sure and simple way to improve psychological science. *Perspectives on Psychological Science*. 2012;7(6):608–614. doi:[10.1177/1745691612462586](https://draft.blogger.com/#)







<!--EndFragment-->